package cluster

import (
	"context"
	"errors"
	"fmt"

	"github.com/go-logr/logr"
	eventhandler "github.com/spotinst/ocean-operator/internal/handler"
	"github.com/spotinst/ocean-operator/internal/handler/cluster"
	"github.com/spotinst/ocean-operator/internal/spot"
	ctrlutil "github.com/spotinst/ocean-operator/internal/util/controller"
	oceanv1 "github.com/spotinst/ocean-operator/pkg/apis/ocean/v1"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/controller"
	"sigs.k8s.io/controller-runtime/pkg/event"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	logf "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/manager"
	"sigs.k8s.io/controller-runtime/pkg/predicate"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
	"sigs.k8s.io/controller-runtime/pkg/source"
)

var (
	// controllerName is the name of the controller.
	controllerName = "controller-cluster"

	// finalizerName is the name of the finalizer.
	finalizerName = fmt.Sprintf("finalizer.%s", oceanv1.SchemeGroupVersion.Group)

	// log is the internal logger used by the controller.
	log = logf.Log.WithName(controllerName)
)

// Add creates a new Cluster Controller and adds it to the Manager. The Manager
// will set fields on the Controller and Start it when the Manager is Started.
func Add(mgr manager.Manager) error {
	return add(mgr, newReconciler(mgr))
}

// newReconciler returns a new handleEvent.ReconcileBase.
func newReconciler(mgr manager.Manager) reconcile.Reconciler {
	return &ReconcileCluster{
		ReconcileBase: ctrlutil.NewReconcile(
			mgr.GetClient(),
			mgr.GetScheme(),
			mgr.GetEventRecorderFor(controllerName)),
	}
}

// add adds a new Controller to mgr with r as the handleEvent.ReconcileBase.
func add(mgr manager.Manager, r reconcile.Reconciler) error {
	// Create a new controller.
	c, err := controller.New(controllerName, mgr, controller.Options{Reconciler: r})
	if err != nil {
		return err
	}

	// Ignore status-only and metadata-only updates.
	predicates := []predicate.Predicate{
		predicate.GenerationChangedPredicate{},
	}

	// Watch for changes to primary resource Cluster.
	if err = c.Watch(
		&source.Kind{Type: &oceanv1.Cluster{}},
		&handler.EnqueueRequestForObject{},
		predicates...); err != nil {
		return err
	}

	return nil
}

// Blank assignment to verify that ReconcileCluster implements handleEvent.ReconcileBase.
var _ reconcile.Reconciler = &ReconcileCluster{}

// ReconcileCluster reconciles a Cluster object.
type ReconcileCluster struct {
	ctrlutil.ReconcileBase
}

// ReconcileRequestContext represents a handleEvent request context.
type ReconcileRequestContext struct {
	ctrlutil.RequestContextBase

	// State.
	clusterLocal    *oceanv1.Cluster // local state
	clusterUpstream *oceanv1.Cluster // upstream state

	// Helpers.
	log     logr.Logger
	handler eventhandler.EventHandler
	ocean   spot.Ocean
}

// Reconcile reads that state of the cluster for a Cluster object and makes
// changes based on the state read and what is in the Cluster.Spec.
//
// The controller will requeue the Request to be processed again if the returned
// error is non-nil or Result.Requeue is true, otherwise upon completion it will
// remove the work from the queue.
func (r *ReconcileCluster) Reconcile(req reconcile.Request) (reconcile.Result, error) {
	// Initialize a new request context.
	ctx, err := r.newContext(req)
	if err != nil {
		return ctrlutil.RequeueIfError(err)
	}
	defer ctx.Cancel()

	// Reconcile handlers.
	handlers := []func(*ReconcileRequestContext) error{
		r.handleSync,              // fetch and sync resources
		r.handleFinalizerAddition, // add finalizer on create/update
		r.handleEvent,             // reconcile resources
		r.handleFinalizerRemoval,  // remove finalizer on delete
	}

	// Reconcile.
	for _, fn := range handlers {
		// Process the request and handle any error.
		if err := fn(ctx); err != nil {
			return ctrlutil.RequeueIfError(r.handleCompletion(ctx, err))
		}

		// Should we continue?
		if ctx.Canceled() {
			// The handler signaled us to stop the reconcile by canceling the
			// request context. If we continue here, it will cause two
			// reconciliation loops since a new request was generated by the
			// handler that changed the object (for example, adding or removing
			// a finalizer). To avoid this, we stop this loop, return here and
			// rely on the new request to reconcile the object.
			return ctrlutil.NoRequeue()
		}
	}

	// Done.
	return ctrlutil.RequeueIfError(r.handleCompletion(ctx, nil))
}

func (r *ReconcileCluster) handleSync(ctx *ReconcileRequestContext) error {
	ctx.log.Info("Handling reconcile sync")

	// Retrieve the local cluster.
	ctx.log.V(1).Info("Retrieving local cluster by namespaced name")
	if err := r.GetClient().Get(ctx, ctx.GetRequest().NamespacedName, ctx.clusterLocal); err != nil {
		if apierrors.IsNotFound(err) {
			ctx.log.Info("Cluster resource not found; ignoring since object must be deleted")
		}

		return fmt.Errorf("failed to retrieve cluster resource: %w", err)
	}

	// Retrieve the upstream cluster.
	if oceanID := ctx.clusterLocal.Status.OceanID; oceanID != "" {
		ctx.log.V(1).Info("Retrieving upstream cluster by ID", "clusterID", oceanID)

		c, err := ctx.ocean.GetCluster(ctx, oceanID)
		if err != nil {
			return fmt.Errorf("failed to retrieve upstream cluster: %w", err)
		}

		// Convert back to ocean/v1.Cluster to make our lives easier later.
		ctx.clusterUpstream, err = ctx.ocean.NewClusterConverter().ToObject(c)
		if err != nil {
			return fmt.Errorf("failed to convert upstream cluster: %w", err)
		}
	}

	return nil
}

func (r *ReconcileCluster) handleEvent(ctx *ReconcileRequestContext) error {
	ctx.log.Info("Handling reconcile event")

	// Determine event type.
	evt := r.newEvent(ctx)
	if evt == nil { // no action required
		return nil
	}

	// Handle event.
	switch e := evt.(type) {

	// Invoke create handler.
	case event.CreateEvent:
		{
			o, err := ctx.handler.OnCreate(ctx, e)
			if err != nil {
				return err
			}

			ctx.clusterUpstream = o.(*oceanv1.Cluster)
			r.RecordEventf(ctx.clusterLocal, corev1.EventTypeNormal,
				"SuccessfulCreate", "created cluster: %v",
				ctx.clusterUpstream.Status.OceanID)

			return nil
		}

	// Invoke update handler.
	case event.UpdateEvent:
		{
			o, err := ctx.handler.OnUpdate(ctx, e)
			if err != nil {
				return err
			}

			ctx.clusterUpstream = o.(*oceanv1.Cluster)
			r.RecordEventf(ctx.clusterLocal, corev1.EventTypeNormal,
				"SuccessfulUpdate", "updated cluster: %v",
				ctx.clusterUpstream.Status.OceanID)

			return nil
		}

	// Invoke delete handler.
	case event.DeleteEvent:
		{
			return ctx.handler.OnDelete(ctx, e)
		}

	default:
		return fmt.Errorf("unsupported event type: %v", e)
	}
}

func (r *ReconcileCluster) handleFinalizerAddition(ctx *ReconcileRequestContext) error {
	hasDeletionTimestamp := ctrlutil.IsBeingDeleted(ctx.clusterLocal)
	hasFinalizer := ctrlutil.HasFinalizer(ctx.clusterLocal, finalizerName)

	if hasDeletionTimestamp || hasFinalizer {
		return nil
	}

	ctx.log.V(1).Info("Adding finalizer")
	prevGeneration := ctx.clusterLocal.GetGeneration()

	ctrlutil.AddFinalizer(ctx.clusterLocal, finalizerName)
	if err := r.UpdateResource(ctx, ctx.clusterLocal); err != nil {
		return fmt.Errorf("failed to added finalizer: %w", err)
	}

	ctx.log.V(1).Info("Finalizer has been added")
	if prevGeneration != ctx.clusterLocal.GetGeneration() {
		// Cancel immediately if the object generation has changed. This prevents
		// some cases where two reconciliation loops will occur.
		ctx.Cancel()
	}

	return nil
}

func (r *ReconcileCluster) handleFinalizerRemoval(ctx *ReconcileRequestContext) error {
	hasDeletionTimestamp := ctrlutil.IsBeingDeleted(ctx.clusterLocal)
	hasFinalizer := ctrlutil.HasFinalizer(ctx.clusterLocal, finalizerName)

	if !hasDeletionTimestamp || !hasFinalizer {
		return nil
	}

	ctx.log.V(1).Info("Removing finalizer")
	prevGeneration := ctx.clusterLocal.GetGeneration()

	ctrlutil.RemoveFinalizer(ctx.clusterLocal, finalizerName)
	if err := r.UpdateResource(ctx, ctx.clusterLocal); err != nil {
		return fmt.Errorf("failed to remove finalizer: %w", err)
	}

	ctx.log.V(1).Info("Finalizer has been removed")
	if prevGeneration != ctx.clusterLocal.GetGeneration() {
		// Cancel immediately if the object generation has changed. This prevents
		// some cases where two reconciliation loops will occur.
		ctx.Cancel()
	}

	return nil
}

func (r *ReconcileCluster) handleCompletion(ctx *ReconcileRequestContext, err error) error {
	ctx.log.Info("Handling reconcile completion")

	// Ignore deleted resources and not found errors.
	if ctrlutil.IsBeingDeleted(ctx.clusterLocal) ||
		apierrors.IsNotFound(errors.Unwrap(err)) {
		return nil
	}

	// Record a new event in case of failure.
	if err != nil {
		r.RecordEvent(ctx.clusterLocal, corev1.EventTypeWarning,
			"FailedReconcile", err.Error())
	}

	// Update the status, if needed.
	if updateErr := r.updateStatus(ctx, err); updateErr != nil {
		return updateErr
	}

	// Return the original error.
	return err
}

func (r *ReconcileCluster) newContext(req reconcile.Request) (*ReconcileRequestContext, error) {
	// Initialize a new base context.
	ctx := context.Background()

	// Generate a new request ID.
	reqID := ctrlutil.NewRequestId()

	// Initialize a new request logger.
	reqLogger := ctrlutil.NewRequestLogger(log, req, reqID)

	// Initialize a new request context.
	reqCtx := ctrlutil.NewRequestContext(ctx, req, reqID, reqLogger)

	// Initialize a new Spot client.
	spotClient, err := ctrlutil.NewRequestSpotClient(reqCtx, r.GetClient())
	if err != nil {
		return nil, err
	}

	// Initialize a new Ocean service.
	oceanService, err := spotClient.Services().Ocean(spot.CloudProviderAWS)
	if err != nil {
		return nil, err
	}

	// Initialize a new event handler.
	reqHandler := &cluster.EventHandler{
		Log:   reqLogger,
		Ocean: oceanService,
	}

	// Return a new request context.
	return &ReconcileRequestContext{
		RequestContextBase: reqCtx,
		ocean:              oceanService,
		clusterLocal:       new(oceanv1.Cluster),
		log:                reqLogger,
		handler:            reqHandler,
	}, nil
}

func (r *ReconcileCluster) newEvent(ctx *ReconcileRequestContext) eventhandler.Event {
	m := ctx.clusterLocal.GetObjectMeta()
	o := ctx.clusterLocal
	u := ctx.clusterUpstream

	// Marked for deletion?
	if ctrlutil.IsBeingDeleted(o) {
		return event.DeleteEvent{Meta: m, Object: o}
	}

	// If there is no upstream cluster, then let's create it.
	if u == nil {
		return event.CreateEvent{Meta: m, Object: o}
	}

	// Compare local and upstream clusters.
	if comp := ctrlutil.CompareClusterSpecs(o.Spec, u.Spec); !comp.Equal {
		return event.UpdateEvent{MetaNew: m, ObjectNew: o}
	}

	return nil
}

func (r *ReconcileCluster) newCondition(ctx *ReconcileRequestContext, err error) oceanv1.StatusCondition {
	var condition oceanv1.StatusCondition

	if err != nil {
		condition = ctrlutil.NewCondition(
			oceanv1.ConditionTypeReady,
			corev1.ConditionFalse,
			"",
			err.Error())
	} else {
		condition = ctrlutil.NewCondition(
			oceanv1.ConditionTypeReady,
			corev1.ConditionTrue,
			"",
			"")
	}

	return condition
}

func (r *ReconcileCluster) newStatus(ctx *ReconcileRequestContext, err error) oceanv1.ClusterStatus {
	// Make copies so that we never mutate the shared informer cache.
	clusterLocal := ctx.clusterLocal.DeepCopy()
	clusterUpstream := ctx.clusterUpstream.DeepCopy()

	// New status.
	newStatus := oceanv1.ClusterStatus{
		ReconcileStatus: oceanv1.ReconcileStatus{
			ObservedGeneration: clusterLocal.Generation,
			Details: oceanv1.StatusDetails{
				ReconcileRequestUID: types.UID(ctx.GetRequestId()),
			},
		},
	}

	// Copy existing conditions one by one so we won't mutate the original object.
	conditions := clusterLocal.Status.Conditions
	for i := range conditions {
		newStatus.Conditions = append(newStatus.Conditions, conditions[i])
	}

	// Create or update a Ready condition.
	ctrlutil.AddCondition(&newStatus.ReconcileStatus, r.newCondition(ctx, err))

	// Upstream information.
	if clusterUpstream != nil {
		newStatus.OceanID = clusterUpstream.Status.OceanID

		if capacity := clusterUpstream.Spec.Capacity; capacity != nil {
			newStatus.Nodes = oceanv1.NodesStatus{
				Current: capacity.Target,
				Minimum: capacity.Minimum,
				Maximum: capacity.Maximum,
			}
		}
	}

	return newStatus
}

func (r *ReconcileCluster) updateStatus(ctx *ReconcileRequestContext, err error) error {
	// Make copies so that we never mutate the shared informer cache.
	oldObj := ctx.clusterLocal.DeepCopy()
	newObj := ctx.clusterLocal.DeepCopy()

	// Build the next status object.
	newObj.Status = r.newStatus(ctx, err)

	// Skip a write if we wouldn't need to update.
	if comp := ctrlutil.CompareClusterStatuses(oldObj.Status, newObj.Status); !comp.Equal {
		ctx.log.V(1).Info("Updating status", "status", newObj.Status)
		if err := r.UpdateStatus(ctx, newObj); err != nil {
			return err
		}

		ctx.log.V(1).Info("Status has been updated")
	}

	return nil
}
